# ğŸ¬ Sistema de AnÃ¡lise de VÃ­deo - Tech Challenge 4

Sistema completo para anÃ¡lise automatizada de vÃ­deos com detecÃ§Ã£o de cenas, emoÃ§Ãµes e atividades humanas usando Deep Learning.

---

## âš¡ InÃ­cio RÃ¡pido (3 Passos)

```bash
# 1. Criar ambientes virtuais (15-25 min)
python setup_dual_environments.py

# 2. Colocar seu vÃ­deo
# Copie para: videos/video.mp4

# 3. Executar pipeline completo
python run_pipeline.py
```

**Requisitos:** Python 3.11 | GPU NVIDIA recomendada (opcional)

---

## ğŸ“‹ Ãndice

- [VisÃ£o Geral](#-visÃ£o-geral)
- [Arquitetura](#-arquitetura)
- [Requisitos](#-requisitos)
- [InstalaÃ§Ã£o](#-instalaÃ§Ã£o)
- [Uso RÃ¡pido](#-uso-rÃ¡pido)
- [Estrutura do Projeto](#-estrutura-do-projeto)
- [Pipeline Completo](#-pipeline-completo)
- [Ambientes Virtuais](#-ambientes-virtuais)
- [Modelos de IA](#-modelos-de-ia)
- [ConfiguraÃ§Ã£o AvanÃ§ada](#-configuraÃ§Ã£o-avanÃ§ada)
- [Troubleshooting](#-troubleshooting)
- [Desenvolvimento](#-desenvolvimento)

---

## ğŸ¯ VisÃ£o Geral

Este projeto analisa vÃ­deos em trÃªs etapas:

### Etapa 1: DetecÃ§Ã£o de Cenas
- Segmenta o vÃ­deo automaticamente em cenas distintas
- Detecta mudanÃ§as de conteÃºdo e transiÃ§Ãµes
- Exporta cada cena como arquivo separado

### Etapa 2: AnÃ¡lise de Sentimentos
- Detecta faces usando MediaPipe e DeepFace
- Classifica emoÃ§Ãµes: Feliz, Triste, Raiva, Surpreso, Neutro, Medo, Nojo
- Gera vÃ­deos anotados com emoÃ§Ãµes e relatÃ³rios detalhados

### Etapa 3: InterpretaÃ§Ã£o de Atividades
TrÃªs mÃ©todos disponÃ­veis:

**ğŸ”€ HÃ­brido (Recomendado)**
- VideoMAE para atividades dinÃ¢micas: DanÃ§ando, Acenando, Fazendo Caretas
- YOLO Pose para atividades estÃ¡ticas: Trabalhando, Lendo, Telefone
- Combina o melhor de cada mÃ©todo

**ğŸ¤– VideoMAE**
- Modelo de transformer para reconhecimento de aÃ§Ãµes
- Ã“timo para movimentos complexos

**ğŸ¯ AnÃ¡lise de Pose (YOLO)**
- DetecÃ§Ã£o de pose + objetos
- Ã“timo para atividades com objetos especÃ­ficos

---

## ğŸ—ï¸ Arquitetura

O projeto utiliza **3 ambientes virtuais isolados** para evitar conflitos de dependÃªncias:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  VÃ­deo de Entrada                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   ETAPA 1: Cenas      â”‚
         â”‚   venv_scenes         â”‚
         â”‚   â€¢ OpenCV            â”‚
         â”‚   â€¢ SceneDetect       â”‚
         â”‚   â€¢ NumPy, SciPy      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ ETAPA 2: EmoÃ§Ãµes      â”‚
         â”‚ venv_emotions         â”‚
         â”‚ â€¢ TensorFlow 2.20+    â”‚
         â”‚ â€¢ DeepFace            â”‚
         â”‚ â€¢ MediaPipe           â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ ETAPA 3: Atividades   â”‚
         â”‚ venv_activities       â”‚
         â”‚ â€¢ PyTorch             â”‚
         â”‚ â€¢ VideoMAE            â”‚
         â”‚ â€¢ YOLO11              â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  VÃ­deos + RelatÃ³rios  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’» Requisitos

### Sistema Operacional
- âœ… Windows 10/11
- âœ… Linux (Ubuntu 20.04+)
- âœ… macOS (Intel/Apple Silicon)

### Software ObrigatÃ³rio
- **Python 3.11** (obrigatÃ³rio - nÃ£o use 3.12 ou 3.13)
- **Git** (para clonar o repositÃ³rio)

### Hardware Recomendado
- **GPU NVIDIA** com suporte CUDA (altamente recomendado)
  - CUDA 11.x ou 12.x
  - 6+ GB VRAM
  - Drivers NVIDIA atualizados
- **CPU**: 8+ cores (alternativa sem GPU, muito mais lento)
- **RAM**: 16+ GB
- **Disco**: 15+ GB livres (modelos + ambientes)

### Hardware MÃ­nimo
- CPU: 4 cores
- RAM: 8 GB
- Disco: 10 GB

> âš ï¸ **Nota**: Sem GPU, o processamento serÃ¡ 10-30x mais lento.

---

## ğŸš€ InstalaÃ§Ã£o

> âš ï¸ **IMPORTANTE**: Siga os passos nesta ordem exata!

### 1. Clonar o RepositÃ³rio

```bash
git clone <url-do-repositorio>
cd TechChallenge4
```

### 2. Verificar/Instalar Python 3.11

**Verificar se jÃ¡ tem:**
```bash
python --version
# Deve mostrar: Python 3.11.x
```

**Se nÃ£o tiver Python 3.11:**

#### Windows
```bash
# Download do instalador
https://www.python.org/downloads/release/python-3119/

# Durante instalaÃ§Ã£o:
# âœ… Marcar "Add Python 3.11 to PATH"
```

#### Linux (Ubuntu/Debian)
```bash
sudo apt update
sudo apt install python3.11 python3.11-venv python3.11-dev
```

#### macOS
```bash
brew install python@3.11
```

### 3. Configurar Ambientes Virtuais (OBRIGATÃ“RIO)

O projeto usa **trÃªs ambientes virtuais separados** para evitar conflitos de dependÃªncias.

#### InstalaÃ§Ã£o AutomÃ¡tica (Recomendado)

```bash
python setup_dual_environments.py
```

**O que o script faz:**
1. âœ… Detecta Python 3.11 automaticamente
2. âœ… Identifica sua GPU NVIDIA (modelo, driver, versÃ£o CUDA)
3. âœ… Cria `venv_scenes` com OpenCV + SceneDetect (leve, 2-3 min)
4. âœ… Cria `venv_emotions` com TensorFlow + DeepFace + MediaPipe
5. âœ… Cria `venv_activities` com PyTorch + VideoMAE + YOLO
6. âœ… Instala pacotes CUDA corretos (11.x ou 12.x) baseado na sua GPU
7. âœ… Testa os trÃªs ambientes e confirma funcionamento da GPU

**Durante a instalaÃ§Ã£o:**
- Pressione `Enter` ou `s` quando solicitado
- Se ambientes jÃ¡ existirem, escolha se quer recriar
- Downloads: ~4-5 GB de pacotes
- Tempo total: 15-25 minutos (depende da conexÃ£o)

**SaÃ­da esperada:**
```
[1/8] Verificando Python 3.11...
    OK: py -3.11
    VersÃ£o: Python 3.11.9

[2/8] Detectando GPU NVIDIA e CUDA...
    OK: GPU NVIDIA detectada
    Modelo: NVIDIA GeForce RTX 3080
    Driver: 537.13
    CUDA suportado: 12.2
    â†’ Instalando pacotes para CUDA 12.x

[3-6/8] Criando ambientes e instalando pacotes...
    âœ… venv_emotions criado
    âœ… venv_activities criado

[7/8] Testando ambientes...
    âœ… TensorFlow: 2.20.0 | GPUs: 1
    âœ… PyTorch: 2.5.1+cu121 | CUDA: True

[8/8] INSTALAÃ‡ÃƒO CONCLUÃDA!
```

#### InstalaÃ§Ã£o Manual

Se preferir controle total:

```bash
# Ambiente 1: EmoÃ§Ãµes
python3.11 -m venv venv_emotions

# Windows
venv_emotions\Scripts\activate
pip install -r requirements_emotions.txt

# Linux/Mac
source venv_emotions/bin/activate
pip install -r requirements_emotions.txt

# Ambiente 2: Atividades
python3.11 -m venv venv_activities

# Windows
venv_activities\Scripts\activate
pip install -r requirements_activities.txt

# Linux/Mac
source venv_activities/bin/activate
pip install -r requirements_activities.txt
```

### 4. Verificar InstalaÃ§Ã£o (Opcional)

**Windows - Menu Interativo:**
```bash
manage_envs.bat
# OpÃ§Ã£o 4: Testar ambos os ambientes
```

**Manual - Testar TensorFlow:**
```bash
venv_emotions\Scripts\python -c "import tensorflow as tf; print(f'TensorFlow: {tf.__version__}'); print(f'GPU: {tf.config.list_physical_devices(\"GPU\")}')"
```

**Manual - Testar PyTorch:**
```bash
venv_activities\Scripts\python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA: {torch.cuda.is_available()}')"
```

**âœ… Tudo OK se mostrar:**
- VersÃµes instaladas (TensorFlow 2.20+, PyTorch 2.0+)
- GPU detectada (se vocÃª tiver NVIDIA)

---

## âš¡ Uso RÃ¡pido

> ğŸ’¡ **Dica**: ApÃ³s instalar os ambientes, tudo Ã© automÃ¡tico!

### Pipeline Completo (Recomendado)

1. **Coloque seu vÃ­deo** em `videos/video.mp4`
   ```bash
   # Copie seu vÃ­deo para a pasta videos
   # Exemplo: videos/video.mp4
   ```

2. **Execute o pipeline**:
   ```bash
   python run_pipeline.py
   ```

   O script irÃ¡:
   - âœ… Verificar se ambientes existem
   - âœ… Alternar automaticamente entre ambientes conforme necessÃ¡rio
   - âœ… Executar as 3 etapas sequencialmente

3. **Escolha o mÃ©todo** para Etapa 3:
   - `1` - HÃ­brido (recomendado)
   - `2` - VideoMAE
   - `3` - AnÃ¡lise de Pose

4. **Resultados** estarÃ£o em:
   - `output/cenas/` - Cenas detectadas
   - `output/sentimentos/` - AnÃ¡lise de emoÃ§Ãµes
   - `output/hibrido/` ou `output/videomae/` - Atividades

### Executar Etapas Individualmente

> ğŸ’¡ **Importante**: Cada etapa usa seu prÃ³prio ambiente virtual!

#### Etapa 1: DetecÃ§Ã£o de Cenas

Usa: `venv_scenes` (leve, sem deep learning)

```bash
# Windows
venv_scenes\Scripts\python.exe src/cli/etapa1_separar_cenas.py --input videos/video.mp4

# Linux/Mac
venv_scenes/bin/python src/cli/etapa1_separar_cenas.py --input videos/video.mp4
```

**OpÃ§Ãµes disponÃ­veis:**
```bash
--input         # VÃ­deo de entrada (padrÃ£o: videos/video.mp4)
--output        # Pasta de saÃ­da (padrÃ£o: output/cenas)
--threshold     # Sensibilidade de detecÃ§Ã£o (padrÃ£o: 27.0)
--min-duration  # DuraÃ§Ã£o mÃ­nima da cena em segundos (padrÃ£o: 1.0)
```

**SaÃ­da:**
- `output/cenas/cena_001.mp4`, `cena_002.mp4`, etc.
- `output/cenas/thumbnails/` - Miniaturas das cenas
- `output/cenas/cenas_metadata.json` - Metadados
- `output/cenas/relatorio_cenas.md` - RelatÃ³rio

---

#### Etapa 2: AnÃ¡lise de Sentimentos

Usa: `venv_emotions` (TensorFlow + DeepFace + MediaPipe)

```bash
# Windows
venv_emotions\Scripts\python.exe src/cli/etapa2_analisar_sentimentos.py

# Linux/Mac
venv_emotions/bin/python src/cli/etapa2_analisar_sentimentos.py
```

**OpÃ§Ãµes disponÃ­veis:**
```bash
--input-dir     # Pasta com cenas (padrÃ£o: output/cenas)
--output-dir    # Pasta de saÃ­da (padrÃ£o: output/sentimentos)
--device        # Dispositivo: cuda, cpu, auto (padrÃ£o: auto)
--fps           # Frames por segundo a analisar (padrÃ£o: 6)
```

**SaÃ­da:**
- `output/sentimentos/cena_001_sentimentos.mp4`, etc.
- `output/sentimentos/relatorio_sentimentos.md` - RelatÃ³rio detalhado
- VÃ­deos anotados com emoÃ§Ãµes detectadas

---

#### Etapa 3: AnÃ¡lise de Atividades

Usa: `venv_activities` (PyTorch + YOLO + VideoMAE)

**3a. MÃ©todo HÃ­brido (Recomendado)**

Combina VideoMAE para atividades dinÃ¢micas e YOLO Pose para estÃ¡ticas:

```bash
# Windows
venv_activities\Scripts\python.exe src/cli/etapa3_hibrido.py

# Linux/Mac
venv_activities/bin/python src/cli/etapa3_hibrido.py
```

**OpÃ§Ãµes disponÃ­veis:**
```bash
--input-dir          # Pasta com cenas (padrÃ£o: output/cenas)
--output-dir         # Pasta de saÃ­da (padrÃ£o: output/hibrido)
--device             # Dispositivo: cuda, cpu, auto (padrÃ£o: auto)
--videomae-conf      # ConfianÃ§a VideoMAE (padrÃ£o: 0.3)
--pose-conf          # ConfianÃ§a Pose (padrÃ£o: 0.5)
--pose-model         # Modelo YOLO Pose (padrÃ£o: models/yolo11x-pose.pt)
--object-model       # Modelo YOLO Objetos (padrÃ£o: models/yolo11x.pt)
```

**Detecta:**
- ğŸ¤– VideoMAE: DanÃ§ando, Acenando, Caretas, Gargalhadas
- ğŸ¯ Pose+Objetos: Trabalhando (laptop), Lendo (livro), Telefone

**SaÃ­da:**
- `output/hibrido/cena_001_hibrido.mp4`, etc.
- `output/hibrido/relatorio_hibrido.md` - RelatÃ³rio

---

**3b. VideoMAE Puro**

Apenas modelo de IA para atividades dinÃ¢micas:

```bash
# Windows
venv_activities\Scripts\python.exe src/cli/etapa3_videomae.py

# Linux/Mac
venv_activities/bin/python src/cli/etapa3_videomae.py
```

**Detecta:** DanÃ§ando, Acenando, Caretas, Gargalhadas, etc.

---

**3c. AnÃ¡lise de Pose Pura**

Apenas YOLO Pose + detecÃ§Ã£o de objetos:

```bash
# Windows
venv_activities\Scripts\python.exe src/cli/etapa3_interpretar_atividades.py

# Linux/Mac
venv_activities/bin/python src/cli/etapa3_interpretar_atividades.py
```

**Detecta:** Trabalhando, Lendo, Telefone

---

### Exemplos de Uso Completo

**Processar vÃ­deo especÃ­fico do inÃ­cio ao fim:**
```bash
# 1. Detectar cenas
venv_scenes\Scripts\python.exe src/cli/etapa1_separar_cenas.py --input videos/meu_video.mp4

# 2. Analisar emoÃ§Ãµes
venv_emotions\Scripts\python.exe src/cli/etapa2_analisar_sentimentos.py

# 3. Detectar atividades (hÃ­brido)
venv_activities\Scripts\python.exe src/cli/etapa3_hibrido.py
```

**Processar apenas algumas cenas especÃ­ficas:**
```bash
# Copie manualmente as cenas desejadas para uma pasta
mkdir output\cenas_selecionadas
copy output\cenas\cena_001.mp4 output\cenas_selecionadas\
copy output\cenas\cena_005.mp4 output\cenas_selecionadas\

# Processe apenas essas cenas
venv_emotions\Scripts\python.exe src/cli/etapa2_analisar_sentimentos.py --input-dir output/cenas_selecionadas
```

**Usar CPU ao invÃ©s de GPU:**
```bash
venv_emotions\Scripts\python.exe src/cli/etapa2_analisar_sentimentos.py --device cpu
venv_activities\Scripts\python.exe src/cli/etapa3_hibrido.py --device cpu
```

---

## ğŸ“ Estrutura do Projeto

```
TechChallenge4/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                          # Este arquivo
â”œâ”€â”€ ğŸ“„ setup_dual_environments.py         # Setup automÃ¡tico
â”œâ”€â”€ ğŸ“„ run_pipeline.py                    # Pipeline completo
â”œâ”€â”€ ğŸ“„ manage_envs.bat                    # Gerenciador (Windows)
â”‚
â”œâ”€â”€ ğŸ“‹ requirements_emotions.txt          # Deps para emoÃ§Ãµes
â”œâ”€â”€ ğŸ“‹ requirements_activities.txt        # Deps para atividades
â”‚
â”œâ”€â”€ ğŸ—‚ï¸ venv_emotions/                     # Ambiente virtual 1
â”‚   â””â”€â”€ TensorFlow + DeepFace + MediaPipe
â”‚
â”œâ”€â”€ ğŸ—‚ï¸ venv_activities/                   # Ambiente virtual 2
â”‚   â””â”€â”€ PyTorch + Transformers + YOLO
â”‚
â”œâ”€â”€ ğŸ—‚ï¸ models/                            # Modelos YOLO
â”‚   â”œâ”€â”€ yolo11x.pt                       # DetecÃ§Ã£o de objetos
â”‚   â””â”€â”€ yolo11x-pose.pt                  # DetecÃ§Ã£o de pose
â”‚
â”œâ”€â”€ ğŸ—‚ï¸ src/                               # CÃ³digo-fonte
â”‚   â”œâ”€â”€ analyzers/                       # Analisadores
â”‚   â”‚   â”œâ”€â”€ emotion_analyzer.py          # DeepFace + MediaPipe
â”‚   â”‚   â”œâ”€â”€ activity_analyzer.py         # YOLO Pose
â”‚   â”‚   â”œâ”€â”€ videomae_analyzer.py         # VideoMAE
â”‚   â”‚   â””â”€â”€ hybrid_analyzer.py           # Combinado
â”‚   â”‚
â”‚   â”œâ”€â”€ activities/                      # Detectores de atividades
â”‚   â”‚   â”œâ”€â”€ reading_activity.py
â”‚   â”‚   â”œâ”€â”€ phone_activity.py
â”‚   â”‚   â”œâ”€â”€ working_activity.py
â”‚   â”‚   â”œâ”€â”€ dancing_activity.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚
â”‚   â”œâ”€â”€ cli/                             # Scripts de linha de comando
â”‚   â”‚   â”œâ”€â”€ etapa1_separar_cenas.py
â”‚   â”‚   â”œâ”€â”€ etapa2_analisar_sentimentos.py
â”‚   â”‚   â”œâ”€â”€ etapa3_interpretar_atividades.py
â”‚   â”‚   â”œâ”€â”€ etapa3_videomae.py
â”‚   â”‚   â””â”€â”€ etapa3_hibrido.py
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                            # NÃºcleo do sistema
â”‚   â”‚   â”œâ”€â”€ video_processor.py
â”‚   â”‚   â””â”€â”€ report_generator.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/                           # UtilitÃ¡rios
â”‚       â”œâ”€â”€ config.py
â”‚       â”œâ”€â”€ logger.py
â”‚       â””â”€â”€ progress_bar.py
â”‚
â””â”€â”€ ğŸ—‚ï¸ output/                            # Resultados
    â”œâ”€â”€ cenas/                           # Etapa 1
    â”œâ”€â”€ sentimentos/                     # Etapa 2
    â”œâ”€â”€ atividades/                      # Etapa 3 (Pose)
    â”œâ”€â”€ videomae/                        # Etapa 3 (VideoMAE)
    â””â”€â”€ hibrido/                         # Etapa 3 (HÃ­brido)
```

---

## ğŸ”„ Pipeline Completo

### Como Funciona

O `run_pipeline.py` executa todas as etapas sequencialmente, **alternando automaticamente entre os ambientes virtuais**:

```python
# Pseudo-cÃ³digo do pipeline
video.mp4
    â†“
[venv_emotions] Etapa 1: Separar Cenas
    â†“ output/cenas/cena_*.mp4
[venv_emotions] Etapa 2: Analisar Sentimentos
    â†“ output/sentimentos/
[venv_activities] Etapa 3: Analisar Atividades
    â†“ output/hibrido/ (ou videomae/ ou atividades/)
```

### OpÃ§Ãµes de Linha de Comando

#### Etapa 1
```bash
python src/cli/etapa1_separar_cenas.py --help

OpÃ§Ãµes:
  --input, -i          VÃ­deo de entrada (padrÃ£o: video.mp4)
  --output-dir, -o     DiretÃ³rio de saÃ­da (padrÃ£o: output/cenas)
  --threshold, -t      Threshold de detecÃ§Ã£o (padrÃ£o: 25.0)
  --min-duration, -m   DuraÃ§Ã£o mÃ­nima da cena em segundos (padrÃ£o: 1.0)
```

#### Etapa 2
```bash
python src/cli/etapa2_analisar_sentimentos.py --help

OpÃ§Ãµes:
  --input-dir, -i      DiretÃ³rio com cenas (padrÃ£o: output/cenas)
  --output-dir, -o     DiretÃ³rio de saÃ­da (padrÃ£o: output/sentimentos)
  --confidence, -c     ConfianÃ§a mÃ­nima (padrÃ£o: 0.5)
  --device, -d         Dispositivo: auto/cuda/cpu (padrÃ£o: auto)
  --no-scores          NÃ£o mostrar scores de confianÃ§a
```

#### Etapa 3 (HÃ­brido)
```bash
python src/cli/etapa3_hibrido.py --help

OpÃ§Ãµes:
  --input-dir, -i      DiretÃ³rio com cenas (padrÃ£o: output/cenas)
  --output-dir, -o     DiretÃ³rio de saÃ­da (padrÃ£o: output/hibrido)
  --device, -d         Dispositivo: auto/cuda/cpu (padrÃ£o: auto)
  --clip-duration      DuraÃ§Ã£o dos clips VideoMAE (padrÃ£o: 2.0s)
  --overlap            Overlap entre clips (padrÃ£o: 1.0s)
```

---

## ğŸ”€ Ambientes Virtuais

### Por que TrÃªs Ambientes?

O projeto utiliza trÃªs frameworks com dependÃªncias conflitantes:
- **Etapa 1**: OpenCV + SceneDetect (leve, sem deep learning)
- **Etapa 2**: TensorFlow 2.20+ (para anÃ¡lise de emoÃ§Ãµes)
- **Etapa 3**: PyTorch (para anÃ¡lise de atividades)

**Conflitos principais:**
- TensorFlow e PyTorch tÃªm versÃµes incompatÃ­veis de `protobuf`
- Conflitos em bibliotecas CUDA entre os frameworks
- CompetiÃ§Ã£o por recursos da GPU
- Etapa 1 nÃ£o precisa do overhead de frameworks de deep learning

**SoluÃ§Ã£o**: Isolar cada etapa em seu prÃ³prio ambiente virtual.

### Estrutura dos Ambientes

#### 1ï¸âƒ£ venv_scenes (DetecÃ§Ã£o de Cenas)
```
PropÃ³sito: Etapa 1 - SeparaÃ§Ã£o de cenas

DependÃªncias principais:
â”œâ”€â”€ opencv-python >= 4.8.0
â”œâ”€â”€ scenedetect[opencv] >= 0.6.0
â”œâ”€â”€ numpy >= 1.24.0
â”œâ”€â”€ scipy >= 1.10.0
â””â”€â”€ pillow >= 10.0.0

Usado por:
â””â”€â”€ src/cli/etapa1_separar_cenas.py

Tempo de instalaÃ§Ã£o: ~2-3 minutos
```

#### 2ï¸âƒ£ venv_emotions (AnÃ¡lise de Sentimentos)
```
PropÃ³sito: Etapa 2 - DetecÃ§Ã£o de emoÃ§Ãµes

DependÃªncias principais:
â”œâ”€â”€ tensorflow >= 2.20.0
â”œâ”€â”€ deepface >= 0.0.79
â”œâ”€â”€ mediapipe >= 0.10.0
â”œâ”€â”€ tf-keras >= 2.20.0
â””â”€â”€ opencv-python, numpy, pillow...

Usado por:
â””â”€â”€ src/cli/etapa2_analisar_sentimentos.py

Tempo de instalaÃ§Ã£o: ~5-8 minutos
```

#### 3ï¸âƒ£ venv_activities (AnÃ¡lise de Atividades)
```
PropÃ³sito: Etapa 3 - DetecÃ§Ã£o de atividades

DependÃªncias principais:
â”œâ”€â”€ torch >= 2.0.0
â”œâ”€â”€ transformers >= 4.57.0
â”œâ”€â”€ ultralytics >= 8.0.0 (YOLO11)
â”œâ”€â”€ protobuf >= 6.33.0
â””â”€â”€ opencv-python, numpy, pillow...

Usado por:
â”œâ”€â”€ src/cli/etapa3_hibrido.py
â”œâ”€â”€ src/cli/etapa3_videomae.py
â””â”€â”€ src/cli/etapa3_interpretar_atividades.py

Tempo de instalaÃ§Ã£o: ~8-12 minutos
```

### Gerenciamento (Windows)

Use o `manage_envs.bat` para facilitar:

```bash
manage_envs.bat
```

Menu interativo:
```
[1] Ativar venv_emotions (para Etapa 2)
[2] Ativar venv_activities (para Etapa 3)
[3] Executar pipeline completo
[4] Testar ambos os ambientes
[5] Verificar versÃµes instaladas
[6] Reinstalar ambientes
[S] Sair
```

### ComparaÃ§Ã£o

| Aspecto         | Ambiente Ãšnico | Ambientes Duais |
|-----------------|----------------|-----------------|
| Conflitos       | âŒ Frequentes  | âœ… Nenhum       |
| InstalaÃ§Ã£o      | âŒ Complexa    | âœ… Simples      |
| ManutenÃ§Ã£o      | âŒ DifÃ­cil     | âœ… FÃ¡cil        |
| Uso de disco    | âœ… ~5 GB       | âš ï¸ ~8 GB        |
| Confiabilidade  | âš ï¸ InstÃ¡vel    | âœ… EstÃ¡vel      |

---

## ğŸ¤– Modelos de IA

### Modelos Utilizados

#### DeepFace (EmoÃ§Ãµes)
```
LocalizaÃ§Ã£o: Baixado automaticamente por DeepFace
DiretÃ³rio: ~/.deepface/weights/

Modelos:
â”œâ”€â”€ retinaface.h5           # DetecÃ§Ã£o de faces
â”œâ”€â”€ facial_expression_model_weights.h5  # EmoÃ§Ãµes
â””â”€â”€ Outros modelos de classificaÃ§Ã£o

Tamanho total: ~200 MB
```

#### MediaPipe (DetecÃ§Ã£o de Faces)
```
LocalizaÃ§Ã£o: Instalado via pip
Modelos embutidos no pacote mediapipe

Funcionalidade:
â””â”€â”€ DetecÃ§Ã£o rÃ¡pida de faces e landmarks
```

#### YOLO11 (Pose e Objetos)
```
LocalizaÃ§Ã£o: models/

Modelos:
â”œâ”€â”€ yolo11x.pt              # DetecÃ§Ã£o de objetos (91 classes COCO)
â”œâ”€â”€ yolo11x-pose.pt         # DetecÃ§Ã£o de pose (17 keypoints)

Tamanho: ~200 MB cada
Download: AutomÃ¡tico na primeira execuÃ§Ã£o
```

**Classes YOLO detectadas**:
- Pessoas
- Objetos: laptop, livro, celular, mouse, teclado
- 91 classes do dataset COCO

#### VideoMAE (Reconhecimento de AÃ§Ãµes)
```
LocalizaÃ§Ã£o: Baixado automaticamente por Transformers
Modelo: MCG-NJU/videomae-base-finetuned-kinetics

Tamanho: ~350 MB
Download: AutomÃ¡tico na primeira execuÃ§Ã£o

Atividades detectadas:
- 400 classes de aÃ§Ãµes (Kinetics-400)
- Selecionadas: DanÃ§ando, Acenando, Fazendo Caretas, etc.
```

### ConfiguraÃ§Ã£o de Modelos

Edite `src/utils/config.py`:

```python
DEFAULT_CONFIG = {
    'activity_analysis': {
        'pose_model': 'models/yolo11x-pose.pt',  # Modelo de pose
        'object_model': 'models/yolo11x.pt',     # Modelo de objetos
        'confidence_threshold': 0.6,
    }
}
```

Ou via linha de comando:

```bash
python src/cli/etapa3_interpretar_atividades.py \
    --pose-model models/yolo11x-pose.pt \
    --object-model models/yolo11x.pt \
    --confidence 0.7
```

### Download Manual de Modelos

Se precisar baixar modelos manualmente:

```bash
# Ativar ambiente
venv_activities\Scripts\activate

# Baixar YOLO11
python -c "from ultralytics import YOLO; YOLO('yolo11x.pt'); YOLO('yolo11x-pose.pt')"

# Mover para pasta models (se necessÃ¡rio)
move yolo11x*.pt models/  # Windows
mv yolo11x*.pt models/    # Linux/Mac
```

---

## âš™ï¸ ConfiguraÃ§Ã£o AvanÃ§ada

### ConfiguraÃ§Ã£o de GPU/CUDA

O `setup_dual_environments.py` detecta automaticamente:

```
GPU Detectada: NVIDIA GeForce RTX 3080
Driver: 537.13
CUDA: 12.2

â†’ Instalando:
  â€¢ TensorFlow 2.20+ com nvidia-cudnn-cu12
  â€¢ PyTorch com cu121 (CUDA 12.1)
```

**CUDA 11.x**:
```bash
# TensorFlow
pip install tensorflow>=2.20.0
pip install nvidia-cudnn-cu11 nvidia-cublas-cu11

# PyTorch
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

**CUDA 12.x**:
```bash
# TensorFlow
pip install tensorflow>=2.20.0
pip install nvidia-cudnn-cu12 nvidia-cublas-cu12

# PyTorch
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
```

**Sem GPU (CPU)**:
```bash
# TensorFlow
pip install tensorflow>=2.20.0

# PyTorch
pip install torch torchvision
```

### Ajustar Performance

#### Para GPUs com Pouca VRAM (<6 GB)

Edite os scripts CLI e reduza batch sizes:

```python
# Em etapa2_analisar_sentimentos.py
result = analyzer.process_scene(
    str(scene_path),
    str(output_path),
    batch_size=8  # Reduzir de 16 para 8
)

# Em etapa3_videomae.py
result = analyzer.process_scene(
    str(scene_path),
    str(output_path),
    batch_size=4  # Reduzir de 8 para 4
)
```

#### Para CPUs

```bash
# Usar menos workers
export NUM_WORKERS=2  # Linux/Mac
set NUM_WORKERS=2     # Windows

# Desabilitar half-precision
# (Adicione ao cÃ³digo se necessÃ¡rio)
```

---

## ğŸ› Troubleshooting

### Problemas Comuns

#### 1. "Ambientes virtuais nÃ£o encontrados"

**Erro:**
```
âš ï¸ AMBIENTES VIRTUAIS NÃƒO ENCONTRADOS
Execute: python setup_dual_environments.py
```

**SoluÃ§Ã£o:**
```bash
# VocÃª precisa criar os ambientes primeiro
python setup_dual_environments.py
```

**Causa:** Os ambientes `venv_scenes`, `venv_emotions` e `venv_activities` nÃ£o foram criados ainda.

#### 2. "CUDA out of memory"

**Causa**: GPU sem memÃ³ria suficiente.

**SoluÃ§Ãµes**:
```bash
# OpÃ§Ã£o 1: Processar vÃ­deos menores
ffmpeg -i video.mp4 -vf scale=640:360 video_small.mp4

# OpÃ§Ã£o 2: Usar CPU
python src/cli/etapa2_analisar_sentimentos.py --device cpu

# OpÃ§Ã£o 3: Reduzir batch size (editar cÃ³digo)
```

#### 3. "TensorFlow nÃ£o detecta GPU"

**Verificar**:
```python
import tensorflow as tf
print(tf.config.list_physical_devices('GPU'))
```

**SoluÃ§Ãµes**:
```bash
# Verificar driver NVIDIA
nvidia-smi

# Reinstalar bibliotecas CUDA
pip uninstall nvidia-cudnn-cu12 nvidia-cublas-cu12
pip install nvidia-cudnn-cu12 nvidia-cublas-cu12

# Verificar versÃ£o TensorFlow
pip install tensorflow>=2.20.0
```

#### 4. "PyTorch nÃ£o detecta GPU"

**Verificar**:
```python
import torch
print(torch.cuda.is_available())
print(torch.version.cuda)
```

**SoluÃ§Ãµes**:
```bash
# Reinstalar PyTorch com CUDA correto
pip uninstall torch torchvision
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
```

#### 5. "ModuleNotFoundError"

**Causa**: Ambiente errado ativado.

**SoluÃ§Ã£o**:
```bash
# Para emoÃ§Ãµes
venv_emotions\Scripts\activate  # Windows
source venv_emotions/bin/activate  # Linux/Mac

# Para atividades
venv_activities\Scripts\activate  # Windows
source venv_activities/bin/activate  # Linux/Mac
```

#### 6. "Modelos YOLO nÃ£o encontrados"

**SoluÃ§Ã£o**:
```bash
# Verificar pasta models
dir models\*.pt  # Windows
ls models/*.pt   # Linux/Mac

# Se vazia, YOLO baixarÃ¡ automaticamente na primeira execuÃ§Ã£o
# Os modelos serÃ£o salvos em models/
```

---

## ğŸ‘¨â€ğŸ’» Desenvolvimento

### Adicionar Nova Atividade

1. Crie arquivo `src/activities/minha_atividade.py`:
```python
from .base_activity import BaseActivity

class MinhaAtividade(BaseActivity):
    def detect(self, pose_data, object_data):
        # Sua lÃ³gica de detecÃ§Ã£o
        if self._check_conditions(pose_data, object_data):
            return True, 0.8  # confidence
        return False, 0.0

    def _check_conditions(self, pose_data, object_data):
        # Implementar verificaÃ§Ãµes
        pass
```

2. Registre em `src/activities/__init__.py`:
```python
from .minha_atividade import MinhaAtividade
```

3. Adicione ao analisador em `src/analyzers/activity_analyzer.py`:
```python
self.activity_detectors = [
    MinhaAtividade(confidence_threshold),
    # ... outros detectores
]
```

---

## ğŸ“Š Benchmarks

### Performance (GPU NVIDIA RTX 3080)

| Etapa | Tempo (1 min vÃ­deo) | VRAM Usada |
|-------|---------------------|------------|
| Etapa 1: Cenas | ~5s | N/A |
| Etapa 2: EmoÃ§Ãµes | ~30s | 2-3 GB |
| Etapa 3: HÃ­brido | ~60s | 4-5 GB |
| **Total** | **~95s** | **5 GB** |

### Performance (CPU Intel i7-10700K)

| Etapa | Tempo (1 min vÃ­deo) |
|-------|---------------------|
| Etapa 1: Cenas | ~10s |
| Etapa 2: EmoÃ§Ãµes | ~8 min |
| Etapa 3: HÃ­brido | ~15 min |
| **Total** | **~23 min** |

> ğŸ“ **Nota**: Tempos variam conforme complexidade do vÃ­deo (nÃºmero de pessoas, movimentos, etc.)

---

## ğŸ“š ReferÃªncias

### Frameworks e Bibliotecas

- **TensorFlow**: https://www.tensorflow.org/
- **PyTorch**: https://pytorch.org/
- **DeepFace**: https://github.com/serengil/deepface
- **MediaPipe**: https://google.github.io/mediapipe/
- **Ultralytics YOLO**: https://docs.ultralytics.com/
- **Transformers (Hugging Face)**: https://huggingface.co/docs/transformers/
- **VideoMAE**: https://github.com/MCG-NJU/VideoMAE

---

## ğŸ”„ AtualizaÃ§Ãµes Recentes

### v2.0.0 (2025-01-02)
- âœ¨ Implementado sistema de ambientes virtuais duais
- âœ¨ DetecÃ§Ã£o automÃ¡tica de GPU e versÃ£o CUDA
- âœ¨ Suporte a YOLO11 (melhor precisÃ£o)
- âœ¨ MÃ©todo hÃ­brido VideoMAE + Pose
- âœ¨ Modelos YOLO organizados em pasta `models/`
- ğŸ› Corrigidos conflitos de dependÃªncias
- ğŸ“š DocumentaÃ§Ã£o consolidada em README Ãºnico

---

**Tech Challenge 4 - FIAP PÃ³s-Tech**

Desenvolvido com â¤ï¸ para anÃ¡lise inteligente de vÃ­deos
